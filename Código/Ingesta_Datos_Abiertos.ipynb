{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba05271-9fd7-4267-9a85-1eab285cb07a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Cuaderno de ingesta de datos\n",
    "En este bloque traeremos la informaci√≥n desde datos abiertos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35dafbfa-4186-4768-b946-78e112c4b44a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos con requests y leerlos en pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000\"\n",
    "url_men = \"https://www.datos.gov.co/resource/nudc-7mev.csv?$limit=100000\"\n",
    " \n",
    "\n",
    "# Descargar contenido\n",
    "response_secop = requests.get(url_secop)\n",
    "response_men = requests.get(url_men)\n",
    "\n",
    "# Convertir contenido a pandas usando StringIO\n",
    "df_secop_pd = pd.read_csv(StringIO(response_secop.text))\n",
    "df_men_pd = pd.read_csv(StringIO(response_men.text))\n",
    "\n",
    "# Convertir pandas a Spark\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "df_men = spark.createDataFrame(df_men_pd)\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_secop)\n",
    "display(df_men)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7e6840-e79a-4559-b588-79e5889a4ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_secop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "df_men.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.men_estadisticas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09aa5437-44e4-4b4c-aecb-78bdd2beec52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 2: Guardar los DataFrames como tablas Delta\n",
    "# La funci√≥n .saveAsTable() guarda los datos y registra la tabla en el Unity Catalog.\n",
    "# El modo \"overwrite\" reemplaza la tabla si ya existe, ideal para actualizaciones.\n",
    "df_secop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.secop\")\n",
    "df_men.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.men_estadisticas\")\n",
    "\n",
    "print(\"¬°Tablas guardadas exitosamente en el cat√°logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8a52835-9e4b-4484-8b14-fcd8b1352ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cargue Datasets Secop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3cf5e0-23a4-476a-bf2f-c5035ae4f208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forzar la columna conflictiva a tipo str antes de pasarlo a Spark\n",
    "df_pd[\"nit_de_la_entidad\"] = df_pd[\"nit_de_la_entidad\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc21998-43ab-41b1-9876-7cafec915384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crear sesi√≥n Spark si no existe\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Leer esquema base de la tabla Delta destino\n",
    "target_schema = spark.table(\"main.diplomado_datos.secop\").schema\n",
    "\n",
    "# Par√°metros para la paginaci√≥n\n",
    "base_url = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000&$offset={offset}\"\n",
    "offset = 0\n",
    "chunk_size = 100000\n",
    "max_rows = 19450000  # N√∫mero estimado de registros\n",
    "chunk_number = 0\n",
    "\n",
    "while offset < max_rows:\n",
    "    print(f\"üì¶ Descargando bloque desde offset {offset}\")\n",
    "\n",
    "    # Construir URL para el bloque\n",
    "    url = base_url.format(offset=offset)\n",
    "\n",
    "    # Descargar el contenido\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Error al descargar datos en offset {offset}\")\n",
    "        break\n",
    "\n",
    "    # Leer CSV con pandas y forzar a string para evitar errores de conversi√≥n\n",
    "    df_pd = pd.read_csv(StringIO(response.text), delimiter=',', header=0, low_memory=False)\n",
    "    df_pd = df_pd.astype(str)\n",
    "\n",
    "    # Verificar si est√° vac√≠o\n",
    "    if df_pd.empty:\n",
    "        print(\"‚úÖ Ingesta completa\")\n",
    "        break\n",
    "\n",
    "    # Convertir a Spark y alinear con el esquema objetivo\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "    df_spark_aligned = df_spark.select([col(f.name).try_cast(f.dataType) for f in target_schema.fields])\n",
    "\n",
    "    # Guardar en Delta Lake en modo append\n",
    "    df_spark_aligned.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "    print(f\"‚úÖ Bloque {chunk_number} ingresado con √©xito\")\n",
    "\n",
    "    # Actualizar offset\n",
    "    offset += chunk_size\n",
    "    chunk_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e24ae9-a9ac-49f3-ace9-23f3502dfb48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar registros ya cargados\n",
    "df_secop = spark.table(\"main.diplomado_datos.secop\")\n",
    "conteo_total = df_secop.count()\n",
    "\n",
    "# Calcular offset siguiente\n",
    "next_offset = (conteo_total // 100000) * 100000\n",
    "print(f\"üëâ Puedes continuar desde offset = {next_offset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9545ba7e-70bb-4fcf-b6dc-145caf03bc66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Crear sesi√≥n Spark si no existe\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Leer esquema base de la tabla Delta destino\n",
    "target_schema = spark.table(\"main.diplomado_datos.secop\").schema\n",
    "\n",
    "# Par√°metros para la paginaci√≥n\n",
    "base_url = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000&$offset={offset}\"\n",
    "offset = 16900000 \n",
    "chunk_size = 100000\n",
    "max_rows = 19549032  # Total datos\n",
    "chunk_number = offset // chunk_size\n",
    "\n",
    "while offset < max_rows:\n",
    "    print(f\"üì¶ Descargando bloque {chunk_number} desde offset {offset}\")\n",
    "    \n",
    "    try:\n",
    "        url = base_url.format(offset=offset)\n",
    "        response = requests.get(url, timeout=30)  # timeout expl√≠cito\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error al descargar datos en offset {offset}: HTTP {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        # Leer CSV como texto plano con pandas\n",
    "        df_pd = pd.read_csv(StringIO(response.text), delimiter=',', header=0, low_memory=False)\n",
    "        df_pd = df_pd.astype(str)  # Forzar todo a string\n",
    "\n",
    "        if df_pd.empty:\n",
    "            print(\"‚úÖ Ingesta finalizada: No hay m√°s registros disponibles.\")\n",
    "            break\n",
    "\n",
    "        # Convertir a Spark DataFrame\n",
    "        df_spark = spark.createDataFrame(df_pd)\n",
    "\n",
    "        # Aplicar try_cast para que errores de conversi√≥n no detengan el proceso\n",
    "        df_spark_aligned = df_spark.select([\n",
    "            expr(f\"try_cast(`{f.name}` as {f.dataType.simpleString()})\").alias(f.name)\n",
    "            for f in target_schema.fields\n",
    "        ])\n",
    "\n",
    "        # Guardar en Delta Lake\n",
    "        df_spark_aligned.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "        print(f\"‚úÖ Bloque {chunk_number} ingresado con √©xito ({len(df_pd)} filas)\")\n",
    "        \n",
    "        offset += chunk_size\n",
    "        chunk_number += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inesperado en offset {offset}: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd8b682-6ba2-454b-bd4b-6832c5577e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_secop = spark.table(\"main.diplomado_datos.secop\")\n",
    "total_registros = df_secop.count()\n",
    "\n",
    "print(f\"üîé Total de registros en 'main.diplomado_datos.secop': {total_registros:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04b5254-3cc3-446c-b48f-3664d98daa52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_secop = spark.table(\"main.diplomado_datos.secop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f275930-75eb-4c3e-9c79-d02fff541f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_secop.limit(15))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Datos_Abiertos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
